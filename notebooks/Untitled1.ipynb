{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d14bd521-a792-441a-9f9e-a3a7900721d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter  # for logging, if you plan to use TensorBoard within Jupyter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from data import load_dataset  # Ensure this module is accessible from your notebook\n",
    "from model import DualStreamTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8b80ea7-0091-4cf0-934a-2ada60828c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2751c96-c91f-40e2-9667-591cc20b74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"protein_vocab_size\": 23,\n",
    "    \"selfies_vocab_size\": 112,\n",
    "    \"embedding_dim\": 128,\n",
    "    \"nhead\": 8,\n",
    "    \"nhid\": 2048,\n",
    "    \"nlayers\": 6,\n",
    "    \"output_dim\": 1,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"epochs\": 10,\n",
    "    \"lr_step_size\": 30,\n",
    "    \"lr_gamma\": 0.1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98774dcf-4331-454f-bea8-78c8fe71a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_dataset(\"../data/raw/Enriched_Set_60percent_similarity.csv\", test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8929f08a-8133-4ded-9939-ad8749a2893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/will/anaconda3/envs/selfseq_env/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = DualStreamTransformer(\n",
    "    protein_vocab_size=config[\"protein_vocab_size\"],\n",
    "    selfies_vocab_size=config[\"selfies_vocab_size\"],\n",
    "    embedding_dim=config[\"embedding_dim\"],\n",
    "    nhead=config[\"nhead\"],\n",
    "    nhid=config[\"nhid\"],\n",
    "    nlayers=config[\"nlayers\"],\n",
    "    output_dim=config[\"output_dim\"]\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "scheduler = StepLR(optimizer, step_size=config[\"lr_step_size\"], gamma=config[\"lr_gamma\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc831bbd-5fc3-451f-99fe-fa05166245a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model, criterion, optimizer, scheduler are already defined as above\n",
    "writer = SummaryWriter()  # TensorBoard summary writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa20e530-18eb-46ea-88a0-d37bcc2f1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # Assuming batch['seq'] returns protein sequences and batch['selfies'] returns SELFIES sequences\n",
    "        protein_seq = batch['seq'].to(device)\n",
    "        selfies_seq = batch['selfies'].to(device)  # Ensure this is the correct key for your SELFIES data\n",
    "        labels = batch['isActive'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        \n",
    "        # Make sure to pass both protein_seq and selfies_seq to the model\n",
    "        outputs = model(protein_seq, selfies_seq)  # Forward pass\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), labels.float())  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % 100 == 99:  # Log every 100 mini-batches\n",
    "            print(f'Epoch [{epoch_index + 1}/{config[\"epochs\"]}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "def validate(epoch_index):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch['seq'].to(device), batch['isActive'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    writer.add_scalar('validation loss', avg_val_loss, epoch_index)\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "193d16bd-5d76-46e1-b49e-3ae4b0c11dea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 780 but got size 350 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     current_val_loss \u001b[38;5;241m=\u001b[39m validate(epoch)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 13\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index)\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Make sure to pass both protein_seq and selfies_seq to the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselfies_seq\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39mfloat())  \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/selfseq_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/selfseq_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/SELFSeq/SELFSeq/notebooks/../src/model.py:72\u001b[0m, in \u001b[0;36mDualStreamTransformer.forward\u001b[0;34m(self, protein_seq, selfies_seq)\u001b[0m\n\u001b[1;32m     69\u001b[0m protein_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotein_hierarchical_attention(protein_encoded\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     70\u001b[0m selfies_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselfies_hierarchical_attention(selfies_encoded\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 72\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselfies_attention\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m combined \u001b[38;5;241m=\u001b[39m combined\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(combined)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 780 but got size 350 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    train_one_epoch(epoch)\n",
    "    current_val_loss = validate(epoch)\n",
    "    print(f\"Epoch {epoch}, Validation Loss: {current_val_loss}\")\n",
    "    \n",
    "    # Checkpointing\n",
    "    if current_val_loss < best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0  # reset counter\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early Stopping\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "    \n",
    "    scheduler.step()  # Adjust learning rate\n",
    "\n",
    "writer.close()  # Close the TensorBoard writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4dafb758-8fda-4482-9f02-70fcd821d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DualStreamTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8801aaf-9b75-4645-a5c8-9dc84272eb58",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 780 but got size 350 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of epochs to train for\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 48\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m validate(model, test_loader, loss_function, device)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[51], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, loss_function, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misActive\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure labels are correctly sized and on the right device\u001b[39;00m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselfies_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/selfseq_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/selfseq_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/SELFSeq/SELFSeq/notebooks/../src/model.py:72\u001b[0m, in \u001b[0;36mDualStreamTransformer.forward\u001b[0;34m(self, protein_seq, selfies_seq)\u001b[0m\n\u001b[1;32m     69\u001b[0m protein_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotein_hierarchical_attention(protein_encoded\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     70\u001b[0m selfies_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselfies_hierarchical_attention(selfies_encoded\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 72\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselfies_attention\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m combined \u001b[38;5;241m=\u001b[39m combined\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     74\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(combined)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 780 but got size 350 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data import load_dataset  # Ensure you import or define your DataLoader\n",
    "\n",
    "# Assuming model is already defined and device is set up\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_function, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        protein_seq = batch['seq'].to(device)\n",
    "        selfies_seq = batch['selfies'].to(device)\n",
    "        labels = batch['isActive'].view(-1, 1).to(device)  # Ensure labels are correctly sized and on the right device\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(protein_seq, selfies_seq)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    return average_loss\n",
    "\n",
    "def validate(model, test_loader, loss_function, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for batch in test_loader:\n",
    "            protein_seq = batch['seq'].to(device)\n",
    "            selfies_seq = batch['selfies'].to(device)\n",
    "            labels = batch['isActive'].view(-1, 1).to(device)\n",
    "\n",
    "            outputs = model(protein_seq, selfies_seq)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    return average_loss\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5  # Number of epochs to train for\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, loss_function, device)\n",
    "    valid_loss = validate(model, test_loader, loss_function, device)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "897587ae-4fdf-4ccd-8000-89370775cd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5442],\n",
      "        [0.5452],\n",
      "        [0.5429],\n",
      "        [0.5435],\n",
      "        [0.5445],\n",
      "        [0.5448],\n",
      "        [0.5449],\n",
      "        [0.5447],\n",
      "        [0.5447],\n",
      "        [0.5445],\n",
      "        [0.5449],\n",
      "        [0.5455],\n",
      "        [0.5450],\n",
      "        [0.5441],\n",
      "        [0.5450],\n",
      "        [0.5436],\n",
      "        [0.5454],\n",
      "        [0.5454],\n",
      "        [0.5444],\n",
      "        [0.5448],\n",
      "        [0.5453],\n",
      "        [0.5449],\n",
      "        [0.5448],\n",
      "        [0.5454],\n",
      "        [0.5458],\n",
      "        [0.5444],\n",
      "        [0.5450],\n",
      "        [0.5444],\n",
      "        [0.5439],\n",
      "        [0.5454],\n",
      "        [0.5459],\n",
      "        [0.5455],\n",
      "        [0.5449],\n",
      "        [0.5445],\n",
      "        [0.5446],\n",
      "        [0.5446],\n",
      "        [0.5448],\n",
      "        [0.5458],\n",
      "        [0.5454],\n",
      "        [0.5454],\n",
      "        [0.5451],\n",
      "        [0.5449],\n",
      "        [0.5456],\n",
      "        [0.5440],\n",
      "        [0.5449],\n",
      "        [0.5450],\n",
      "        [0.5443],\n",
      "        [0.5456],\n",
      "        [0.5463],\n",
      "        [0.5457],\n",
      "        [0.5451],\n",
      "        [0.5447],\n",
      "        [0.5450],\n",
      "        [0.5452],\n",
      "        [0.5462],\n",
      "        [0.5454],\n",
      "        [0.5459],\n",
      "        [0.5461],\n",
      "        [0.5448],\n",
      "        [0.5451],\n",
      "        [0.5450],\n",
      "        [0.5440],\n",
      "        [0.5448],\n",
      "        [0.5456],\n",
      "        [0.5447],\n",
      "        [0.5454],\n",
      "        [0.5448],\n",
      "        [0.5458],\n",
      "        [0.5446],\n",
      "        [0.5449],\n",
      "        [0.5443],\n",
      "        [0.5439],\n",
      "        [0.5446],\n",
      "        [0.5448],\n",
      "        [0.5450],\n",
      "        [0.5447],\n",
      "        [0.5443],\n",
      "        [0.5450],\n",
      "        [0.5434],\n",
      "        [0.5452],\n",
      "        [0.5448],\n",
      "        [0.5434],\n",
      "        [0.5445],\n",
      "        [0.5435],\n",
      "        [0.5441],\n",
      "        [0.5441],\n",
      "        [0.5440],\n",
      "        [0.5450],\n",
      "        [0.5435],\n",
      "        [0.5440],\n",
      "        [0.5446],\n",
      "        [0.5449],\n",
      "        [0.5451],\n",
      "        [0.5444],\n",
      "        [0.5445],\n",
      "        [0.5441],\n",
      "        [0.5446],\n",
      "        [0.5447],\n",
      "        [0.5450],\n",
      "        [0.5453]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Mock Data Creation\n",
    "mock_protein_seq = torch.randint(0, 26, (4, 100))  # (batch_size, seq_len)\n",
    "mock_selfies_seq = torch.randint(0, 115, (4, 100))  # (batch_size, seq_len)\n",
    "mock_protein_seq, mock_selfies_seq = mock_protein_seq.to(device), mock_selfies_seq.to(device)\n",
    "\n",
    "# Forward Pass\n",
    "with torch.no_grad():\n",
    "    mock_output = model(mock_protein_seq, mock_selfies_seq)\n",
    "print(mock_output)  # Check output shapes and values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56795897-bbda-41ce-9ab6-396df7c85f6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m protein_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mprotein_hierarchical_attention(protein_encoded)\n\u001b[1;32m      2\u001b[0m selfies_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselfies_hierarchical_attention(selfies_encoded)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProtein attention shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotein_attention\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Check shape\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bae74-99d1-43c6-9aa9-9465b22f8f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (selfseq_env)",
   "language": "python",
   "name": "selfseq_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
